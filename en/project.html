<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yiwen Zhao's Portfolio</title>
    <link rel="stylesheet" href="./../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body class="resume-page">
    <div class="background"></div>

    <div class="content" style="margin-top: 30px;">
        <nav class="navbar">
            <ul>
                <li class="nav-item"><a href="./../index.html">Home</a></li>
                <li class="nav-item"><a href="./project.html">Pubs/Projects</a></li>
                <li class="nav-item"><a href="./activities.html">Activities</a></li>
                <li class="nav-item">
                    <a href="https://github.com/Tsukasane" target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
                <li class="nav-item">
                    <a href="https://www.linkedin.com/in/yiwen-zhao-466429295" target="_blank">
                        <i class="fab fa-linkedin"></i>
                    </a>
                </li>
                <li class="nav-item">
                    <a href="https://scholar.google.com/citations?authuser=1&user=EEwLuMQAAAAJ" target="_blank">
                        <i class="fas fa-graduation-cap"></i>
                    </a>
                </li>
                <li class="nav-item language-switch">
                    <a href="./index.html" id="switch-en">EN</a>
                    <!-- <a href="./index.html" id="switch-en">EN</a> | <a href="./cn/index_cn.html" id="switch-cn">中文</a> -->
                </li>
            </ul>
        </nav>

        <div id="home" class="section active" style="margin-bottom: 0px;">
            <h1>Projects/Publications</h1>
            <h2>Recent Update: 2025-10-2</h2>
        </div>

        <div class="container" style="margin-top: 0px;"> 
            <h3>Projects</h3>
            
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/t2mhuman.png" alt="t2mhumanfeedback" class="project-photo">
                    <div class="project-title">Fine-tune a motion generative model through human feedback.</div>
                    <div class="project-authors">Integrate human feedback into the text-to-motion model fine-tuning. Guide humanoid motion to better align with human preferences.</div>
                </div>
            </div>

            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/OnlineHMR.png" alt="onlineHMR" class="project-photo">
                    <div class="project-title">Video-based human mesh recovery.</div>
                    <div class="project-authors">Deal with the challenge of online input, world-coordinate human paramatric model streaming inference.</div>
                </div>
            </div>

            <h3>Conference Papers</h3>
            
            <!-- FreeDance Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/freedance.png" alt="FreeDance" class="project-photo">
                    <div class="project-title">FreeDance: Towards Harmonic Free-Number Group Dance Generation via a Unified Framework</div>
                    <div class="project-authors"><b>Yiwen Zhao</b>, <a href="https://yangwangpku.github.io/">Yang Wang</a>, <a href="https://www.linkedin.com/in/liting-wen">Liting Wen</a>, <a href="https://github.com/hengyuan-zhang-0">Hengyuan Zhang</a>, <a href="https://xingqunqi-lab.github.io/QXQPage/">Xingqun Qi</a></div>
                    <div class="project-venue"><b>[ICCV 2025]</b><span class="badge1">Travel Grant</span> | 
                        <a href="https://tsukasane.github.io/FreeDance/">project page</a> | <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhao_FreeDance_Towards_Harmonic_Free-Number_Group_Dance_Generation_via_a_Unified_ICCV_2025_paper.pdf">paper</a> | <a href="https://github.com/Tsukasane/FreeDance">code</a></div>
                    <!-- <div class="project-tldr">A unified framework for generating group dance with any number of dancers, ensuring harmonic coordination and natural movement patterns.</div> -->
                </div>
            </div>

            <!-- Singing Voice Synthesis Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/svsaug.png" alt="SVSAug" class="project-photo">
                    <div class="project-title">Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty</div>
                    <div class="project-authors"><b>Yiwen Zhao</b>, <a href="http://shijt.site/">Jiatong Shi</a>, <a href="https://scholar.google.com/citations?user=YTEG_FIAAAAJ&hl=zh-CN">Yuxun Tang</a>, <a href="https://wanchichen.github.io/">William Chen</a>, <a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></div>
                    <div class="project-venue">
                        <b>[ASRU 2025]</b> | 
                        <a href="https://tsukasane.github.io/SingingUncertainty/">project page</a> | <a href="">paper(coming soon)</a> | <a href="">code(coming soon)</a></div>
                    <!-- <div class="project-tldr">Improving singing voice synthesis robustness by incorporating uncertainty estimation in both prior and posterior distributions during training.</div> -->
                </div>
            </div>
            
            <!-- ARECHO Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/arecho_logo_white.png" alt="ARECHO" class="project-photo">
                    <div class="project-title">ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation</div>
                    <div class="project-authors"><a href="http://shijt.site/">Jiatong Shi</a>, <a href="https://scholar.google.com/citations?user=6DPnbBEAAAAJ&hl=en">Yifan Cheng</a>, <a href="https://scholar.google.com/citations?user=b35P7lAAAAAJ&hl=zh-TW">Bo-Hao Su</a>, <a href="https://scholar.google.co.kr/citations?user=MSIvlNoAAAAJ&hl=en&authuser=1">Hye-jin Shim</a>, <a href="https://jctian98.github.io/">Jinchuan Tian</a>, <a href="https://scholar.google.com/citations?user=A3lfL0QAAAAJ&hl=en">Samuele Cornell</a>, <b>Yiwen Zhao</b>, <a href="https://siddhu001.github.io/">Siddhant Arora</a>, <a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></div>
                    <div class="project-venue">
                        <b>[NeurIPS 2025]</b><span class="badge">Spotlight</span> |
                        <a href="https://arxiv.org/abs/2505.24518">paper</a> |
                        <a href="https://github.com/shinjiwlab/versa">code</a>
                    </div>
                    <!-- <div class="project-tldr">An autoregressive evaluation framework that optimizes speech quality metrics through chain-based hypothesis generation for more accurate multi-metric estimation.</div> -->
                </div>
            </div>

            <!-- ESPnet-SpeechLM Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/espnet_speechlm.png" alt="SpeechLM" class="project-photo">
                    <div class="project-title">ESPnet-SpeechLM: An Open Speech Language Model Toolkit</div>
                    <div class="project-authors"><a href="https://jctian98.github.io/">Jinchuan Tian</a>, <a href="http://shijt.site/">Jiatong Shi</a>, <a href="https://wanchichen.github.io/">William Chen</a>, <a href="https://siddhu001.github.io/">Siddhant Arora</a>, <a href="https://sites.google.com/view/yoshiki-masuyama/home">Yoshiki Masuyama</a>, <a href="https://www.semanticscholar.org/author/Takashi-Maekaku/1816126">Takashi Mackaku</a>, <a href="https://wyh2000.github.io/">Yihan Wu</a>, <a href="https://scholar.google.com/citations?user=tpb8DSQAAAAJ&hl=zh-CN">Junyi Peng</a>, <a href="https://scholar.google.com/citations?user=pbU47_MAAAAJ&hl=en">Shikhar Bharadwaj</a>, <b>Yiwen Zhao</b>, <a href="https://scholar.google.com/citations?user=A3lfL0QAAAAJ&hl=en">Samuele Cornell</a>, <a href="https://pyf98.github.io/">Yifan Peng</a>, <a href="https://xiangyue9607.github.io/">Xiang Yue</a>, <a href="https://huckiyang.github.io/">Chao-Han Huck Yang</a>, <a href="https://phontron.com/">Graham Neubig</a>, <a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></div>
                    <div class="project-venue"><b>[NAACL 2025]</b> (Demo) | <a href="https://arxiv.org/abs/2502.15218">paper</a> | <a href="https://github.com/espnet/espnet/tree/speechlm">code</a></div>
                    <!-- <div class="project-tldr">An open-source toolkit for building and deploying speech language models, providing comprehensive tools for speech understanding and generation tasks.</div> -->
                </div>
            </div>

            <!-- VERSA Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/versa.png" alt="VERSA" class="project-photo">
                    <div class="project-title">VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</div>
                    <div class="project-authors"><a href="http://shijt.site/">Jiatong Shi</a>, <a href="https://scholar.google.co.kr/citations?user=MSIvlNoAAAAJ&hl=en&authuser=1">Hye-jin Shim</a>, <a href="https://jctian98.github.io/">Jinchuan Tian</a>, <a href="https://siddhu001.github.io/">Siddhant Arora</a>, <a href="https://hbwu-ntu.github.io/">Haibin Wu</a>, <a href="https://www.dariuspetermann.com/">Darius Petermann</a>, <a href="https://scholar.google.com/citations?user=FVphZogAAAAJ&hl=en">Jia Qi Yip</a>, <a href="https://yzyouzhang.com/">You Zhang</a>, <a href="https://scholar.google.com/citations?user=YTEG_FIAAAAJ&hl=zh-CN">Yuxun Tang</a>, <a href="https://sites.google.com/view/wangyou-zhang">Wangyou Zhang</a>, <a href="https://dareenharthi.github.io/index.html">Dareen Alharthi</a>, <a href="https://www.yichenwilliamhuang.com/">Yichen Huang</a>, <a href="https://scholar.google.com/citations?user=UT-g5BAAAAAJ&hl=en">Koichi Saito</a>, <a href="https://scholar.google.com/citations?user=KrEYe-IAAAAJ&hl=en">Jionghao Han</a>, <b>Yiwen Zhao</b>, <a href="https://chrisdonahue.com/">Chris Donahue</a>, <a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></div>
                    <div class="project-venue"><b>[NAACL 2025]</b> (Demo) | <a href="https://arxiv.org/abs/2412.17667">paper</a> | <a href="https://github.com/shinjiwlab/versa">code</a></div>
                    <!-- <div class="project-tldr">A comprehensive evaluation toolkit for speech, audio, and music tasks, providing standardized metrics and evaluation pipelines for researchers and practitioners.</div> -->
                </div>
            </div>

            <!-- VR Locomotion Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/vrlocomotion.png" alt="VRLocomotion" class="project-photo">
                    <div class="project-title">Exploring Locomotion Methods with Upright Redirected Views for VR Users in Reclining & Lying Positions</div>
                    <div class="project-authors"><a href="https://scholar.google.com/citations?user=AltFe_0AAAAJ&hl=zh-CN">Tianren Luo</a>, <a href="https://orcid.org/0000-0001-8207-9434">Chenyang Cai</a>, <b>Yiwen Zhao</b>, <a href="https://orcid.org/0000-0002-0425-1590">Yachun Fan</a>, <a href="https://scholar.google.com/citations?user=XldCrGEAAAAJ&hl=en">Zhigeng Pan</a>, <a href="https://scholar.google.com/citations?user=kHKwQ9gAAAAJ&hl=en">Teng Han</a>, <a href="https://lcs.ios.ac.cn/~fengt/">Feng Tian</a></div>
                    <div class="project-venue"><b>[UIST 2023]</b><span class="badge">Oral</span> | 
                        <a href="https://dl.acm.org/doi/abs/10.1145/3586183.3606714">paper</a></div>
                    <!-- <div class="project-tldr">Novel VR locomotion techniques using upright redirected views to enable comfortable navigation for users in reclining and lying positions.</div> -->
                </div>
            </div>

            <h3>Workshop Papers</h3>
            <!-- SLMSVS Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/SLMSVS.png" alt="SLMSVS" class="project-photo">
                    <div class="project-title">Adapting Speech Language Model to Singing Voice Synthesis</div>
                    <div class="project-authors"><b>Yiwen Zhao</b>, <a href="http://shijt.site/">Jiatong Shi</a>, <a href="https://jctian98.github.io/">Jinchuan Tian</a>, <a href="https://scholar.google.com/citations?user=YTEG_FIAAAAJ&hl=zh-CN">Yuxun Tang</a>, <a href="https://haidog-yaqub.github.io/">Jiarui Hai</a>, <a href="https://scholar.google.com/citations?user=KrEYe-IAAAAJ&hl=en">Jionghao Han</a>, <a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></div>
                    <div class="project-venue"><b>[NeurIPSW 2025 AI4Music]</b><span class="badge1">Student Grant</span> | <a href="https://tsukasane.github.io/SLMSVS/">project page</a> | <a href="">paper(coming soon)</a> | <a href="https://github.com/Tsukasane/SLMSVS/tree/master">code</a></div>
                    <!-- <div class="project-tldr">Adapting a Text-to-Speech pretrained speech language model to the low-resource Singing Voice Synthesis task..</div> -->
                </div>
            </div>

            <!-- Fashion Chatroom Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/fashionchatroom.png" alt="FashionChatroom" class="project-photo">
                    <div class="project-title">Fashion Chatroom: An Automated Pipeline for Fashion Dataset Construction</div>
                    <div class="project-authors"><b>Yiwen Zhao</b>, <a href="https://cs.pku.edu.cn/info/1236/2126.htm">Huizhu Jia</a>, <a href="https://pku-hmi-lab.github.io/HMI-Web/leader.html">Shanghang Zhang</a></div>
                    <div class="project-venue"><b>[AAAIW 2024 AI4CSE]</b> | <a href="https://ai-2-ase.github.io/papers/6%5cCameraReady%5cCR_FashionChatroom_AAAIW24AI2ASE.pdf">paper</a></div>
                    <!-- <div class="project-tldr">An automated pipeline for constructing fashion datasets through conversational AI, enabling efficient data collection for fashion-related machine learning tasks.</div> -->
                </div>
            </div>

            <h3>Underreview Papers</h3>
            <!-- CRISP Project -->
            <div class="project-item">
                <div class="project-content">
                    <img src="./../resources/CRISP.png" alt="CRISP" class="project-photo">
                    <div class="project-title">CRISP: Contact-guided Real2Sim from Monocular Video with Planar Scene Primitives</div>
                    <div class="project-authors"><a href="https://z1hanw.github.io/">Zihan Wang</a>, <a href="https://jiashunwang.github.io/">Jiashun Wang</a>, <a href="https://jefftan969.github.io/">Jeff Tan</a>, <b>Yiwen Zhao</b>, <a href="https://www.ri.cmu.edu/ri-faculty/jessica-k-hodgins/">Jessica K. Hodgins</a>, <a href="https://shubhtuls.github.io/">Shubham Tulsiani</a>, <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a></div>
                    <div class="project-venue">in submission</div>
                </div>
            </div>
            
        </div>

    </div>

    <script src="./scripts.js"></script>
</body>
</html>
